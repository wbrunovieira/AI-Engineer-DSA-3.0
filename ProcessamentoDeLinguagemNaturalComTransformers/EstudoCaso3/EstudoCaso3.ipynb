{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Processamento de Linguagem Natural com Transformers</font>\n",
    "\n",
    "## <font color='blue'>Estudo de Caso 3</font>\n",
    "## <font color='blue'>Construindo e Treinando Um Modelo de Linguagem Transformer a Partir do Zero em Português</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DSA](imagens/EC3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.10.9\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark.\n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XQhwlGcdpydo"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "re   : 2.2.1\n",
      "torch: 2.0.1\n",
      "numpy: 1.23.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando os Dados de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados de texto\n",
    "text = open('dados/frases.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Olá, como vai? Eu sou a Ana.\\n'\n",
      "'Olá, Ana, meu nome é Carlos. Muito prazer.\\n'\n",
      "'Prazer em conhecer você também. Como você está hoje?\\n'\n",
      "'Ótimo. Meu time de futebol venceu a competição.\\n'\n",
      "'Uau Parabéns, Carlos!\\n'\n",
      "'Obrigado Ana.\\n'\n",
      "'Vamos comer uma pizza mais tarde para celebrar?\\n'\n",
      "'Claro. Você recomenda algum restaurante Ana?\\n'\n",
      "'Sim, abriu um restaurante novo e dizem que a pizza de banana é fenomenal.\\n'\n",
      "'Ok. Nos encontramos no restaurante às sete da noite, pode ser?\\n'\n",
      "'Pode sim. Nos vemos mais tarde então.'\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-Processamento dos Dados de Texto e Construção do Vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos caracteres especiais: '.', ',', '?', '!'\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'olá como vai eu sou a ana\\\\n'\", \"'olá ana meu nome é carlos muito prazer\\\\n'\", \"'prazer em conhecer você também como você está hoje\\\\n'\", \"'ótimo meu time de futebol venceu a competição\\\\n'\", \"'uau parabéns carlos\\\\n'\", \"'obrigado ana\\\\n'\", \"'vamos comer uma pizza mais tarde para celebrar\\\\n'\", \"'claro você recomenda algum restaurante ana\\\\n'\", \"'sim abriu um restaurante novo e dizem que a pizza de banana é fenomenal\\\\n'\", \"'ok nos encontramos no restaurante às sete da noite pode ser\\\\n'\", \"'pode sim nos vemos mais tarde então'\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos as frases em palavras e criamos uma lista de palavras\n",
    "word_list = list(set(\" \".join(sentences).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'pode\", 'uma', \"então'\", 'abriu', 'nos', 'também', \"ser\\\\n'\", 'banana', 'está', 'time', 'conhecer', \"celebrar\\\\n'\", 'restaurante', 'sim', \"'prazer\", \"'ótimo\", 'sou', 'meu', 'de', 'pizza', \"'claro\", 'recomenda', \"'sim\", 'é', \"'uau\", 'sete', 'futebol', 'tarde', 'em', \"hoje\\\\n'\", 'da', 'venceu', 'vemos', \"'ok\", \"ana\\\\n'\", 'no', 'muito', 'comer', 'mais', 'vai', 'novo', 'que', 'carlos', 'a', 'um', 'encontramos', 'você', 'algum', 'parabéns', 'e', \"'olá\", 'para', 'dizem', 'noite', 'pode', \"carlos\\\\n'\", \"'obrigado\", \"fenomenal\\\\n'\", 'ana', 'como', 'eu', 'nome', 'às', \"'vamos\", \"prazer\\\\n'\", \"competição\\\\n'\"]\n"
     ]
    }
   ],
   "source": [
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o dicionário de palavras com os tokens especiais do BERT\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluímos as palavras no dicionário e criamos índices\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, \"'pode\": 4, 'uma': 5, \"então'\": 6, 'abriu': 7, 'nos': 8, 'também': 9, \"ser\\\\n'\": 10, 'banana': 11, 'está': 12, 'time': 13, 'conhecer': 14, \"celebrar\\\\n'\": 15, 'restaurante': 16, 'sim': 17, \"'prazer\": 18, \"'ótimo\": 19, 'sou': 20, 'meu': 21, 'de': 22, 'pizza': 23, \"'claro\": 24, 'recomenda': 25, \"'sim\": 26, 'é': 27, \"'uau\": 28, 'sete': 29, 'futebol': 30, 'tarde': 31, 'em': 32, \"hoje\\\\n'\": 33, 'da': 34, 'venceu': 35, 'vemos': 36, \"'ok\": 37, \"ana\\\\n'\": 38, 'no': 39, 'muito': 40, 'comer': 41, 'mais': 42, 'vai': 43, 'novo': 44, 'que': 45, 'carlos': 46, 'a': 47, 'um': 48, 'encontramos': 49, 'você': 50, 'algum': 51, 'parabéns': 52, 'e': 53, \"'olá\": 54, 'para': 55, 'dizem': 56, 'noite': 57, 'pode': 58, \"carlos\\\\n'\": 59, \"'obrigado\": 60, \"fenomenal\\\\n'\": 61, 'ana': 62, 'como': 63, 'eu': 64, 'nome': 65, 'às': 66, \"'vamos\": 67, \"prazer\\\\n'\": 68, \"competição\\\\n'\": 69}\n"
     ]
    }
   ],
   "source": [
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invertemos a ordem e colocamos os índices como chave e as palavras como valor no dicionário\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: '[CLS]',\n",
       " 2: '[SEP]',\n",
       " 3: '[MASK]',\n",
       " 4: \"'pode\",\n",
       " 5: 'uma',\n",
       " 6: \"então'\",\n",
       " 7: 'abriu',\n",
       " 8: 'nos',\n",
       " 9: 'também',\n",
       " 10: \"ser\\\\n'\",\n",
       " 11: 'banana',\n",
       " 12: 'está',\n",
       " 13: 'time',\n",
       " 14: 'conhecer',\n",
       " 15: \"celebrar\\\\n'\",\n",
       " 16: 'restaurante',\n",
       " 17: 'sim',\n",
       " 18: \"'prazer\",\n",
       " 19: \"'ótimo\",\n",
       " 20: 'sou',\n",
       " 21: 'meu',\n",
       " 22: 'de',\n",
       " 23: 'pizza',\n",
       " 24: \"'claro\",\n",
       " 25: 'recomenda',\n",
       " 26: \"'sim\",\n",
       " 27: 'é',\n",
       " 28: \"'uau\",\n",
       " 29: 'sete',\n",
       " 30: 'futebol',\n",
       " 31: 'tarde',\n",
       " 32: 'em',\n",
       " 33: \"hoje\\\\n'\",\n",
       " 34: 'da',\n",
       " 35: 'venceu',\n",
       " 36: 'vemos',\n",
       " 37: \"'ok\",\n",
       " 38: \"ana\\\\n'\",\n",
       " 39: 'no',\n",
       " 40: 'muito',\n",
       " 41: 'comer',\n",
       " 42: 'mais',\n",
       " 43: 'vai',\n",
       " 44: 'novo',\n",
       " 45: 'que',\n",
       " 46: 'carlos',\n",
       " 47: 'a',\n",
       " 48: 'um',\n",
       " 49: 'encontramos',\n",
       " 50: 'você',\n",
       " 51: 'algum',\n",
       " 52: 'parabéns',\n",
       " 53: 'e',\n",
       " 54: \"'olá\",\n",
       " 55: 'para',\n",
       " 56: 'dizem',\n",
       " 57: 'noite',\n",
       " 58: 'pode',\n",
       " 59: \"carlos\\\\n'\",\n",
       " 60: \"'obrigado\",\n",
       " 61: \"fenomenal\\\\n'\",\n",
       " 62: 'ana',\n",
       " 63: 'como',\n",
       " 64: 'eu',\n",
       " 65: 'nome',\n",
       " 66: 'às',\n",
       " 67: \"'vamos\",\n",
       " 68: \"prazer\\\\n'\",\n",
       " 69: \"competição\\\\n'\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "# Tamanho do vocabulário\n",
    "vocab_size = len(word_dict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos uma lista para os tokens\n",
    "token_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dZXQM7TIpydr"
   },
   "outputs": [],
   "source": [
    "# Loop pelas sentenças para criar a lista de tokens\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tab7hDxUpyds",
    "outputId": "d95b8e7c-85a3-4c3e-9c89-68525c297b71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[54, 63, 43, 64, 20, 47, 38],\n",
       " [54, 62, 21, 65, 27, 46, 40, 68],\n",
       " [18, 32, 14, 50, 9, 63, 50, 12, 33],\n",
       " [19, 21, 13, 22, 30, 35, 47, 69],\n",
       " [28, 52, 59],\n",
       " [60, 38],\n",
       " [67, 41, 5, 23, 42, 31, 55, 15],\n",
       " [24, 50, 25, 51, 16, 38],\n",
       " [26, 7, 48, 16, 44, 53, 56, 45, 47, 23, 22, 11, 27, 61],\n",
       " [37, 8, 49, 39, 16, 66, 29, 34, 57, 58, 10],\n",
       " [4, 17, 8, 36, 42, 31, 6]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Olá, como vai? Eu sou a Ana.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeira frase\n",
    "text[0:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[54, 63, 43, 64, 20, 47, 38]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeira frase no formato de token (o que será usado para treinar o modelo BERT)\n",
    "token_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Abaixo estão os Hiperparâmetros usados para controlar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_HzEd-kcpydt"
   },
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "batch_size = 6\n",
    "n_segments = 2\n",
    "dropout = 0.2\n",
    "\n",
    "# Comprimento máximo\n",
    "maxlen = 100 \n",
    "\n",
    "# Número máximo de tokens que serão previstos\n",
    "max_pred = 7\n",
    "\n",
    "# Número de camadas \n",
    "n_layers = 6 \n",
    "\n",
    "# Número de cabeças no multi-head attention\n",
    "n_heads = 12\n",
    "\n",
    "# Tamanho da embedding\n",
    "d_model = 768\n",
    "\n",
    "# Tamanho da dimensão feedforward: 4 * d_model\n",
    "d_ff = d_model * 4\n",
    "\n",
    "# Dimensão de K(=Q)V\n",
    "d_k = d_v = 64 \n",
    "\n",
    "# Epochs\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação dos Batches de Dados e Aplicação dos Tokens Especiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função make_batch() abaixo cria lotes (batches) de dados para o treinamento do modelo BERT. Ela é responsável por gerar a entrada correta necessária para o treinamento do BERT, que inclui os tokens de entrada, os tokens mascarados, as posições dos tokens mascarados, os IDs de segmentos e um rótulo indicando se a segunda sentença segue imediatamente a primeira. Vamos descrever cada uma das partes da função e usar imagens para facilitar a compreensão.\n",
    "\n",
    "Inicialização: A função começa inicializando um lote vazio e contadores para sentenças positivas e negativas. Sentenças positivas são pares de sentenças onde a segunda sentença segue imediatamente a primeira, enquanto as negativas são pares onde isso não ocorre. O lote deve ser equilibrado entre sentenças positivas e negativas.\n",
    "\n",
    "Geração de pares de sentenças: Para cada instância no lote, a função seleciona aleatoriamente duas sentenças do conjunto de dados. Cada sentença é então convertida em uma lista de IDs de tokens e os tokens especiais [CLS] e [SEP] são adicionados nos lugares apropriados.\n",
    "\n",
    "Segment IDs: Para cada par de sentenças, a função gera IDs de segmentos, que são 0 para tokens na primeira sentença e 1 para tokens na segunda sentença.\n",
    "\n",
    "Masked Language Model (MLM): A função então seleciona aleatoriamente 15% dos tokens para mascarar para a tarefa de MLM, garantindo que os tokens [CLS] e [SEP] não sejam mascarados. Esses tokens são substituídos pelo token [MASK], por um token aleatório ou permanecem inalterados, dependendo de um sorteio aleatório.\n",
    "\n",
    "![DSA](imagens/bert2.png)\n",
    "\n",
    "Padding: A função adiciona padding aos IDs de entrada, aos IDs de segmento, aos tokens mascarados e às posições mascaradas para garantir que todas as listas tenham o mesmo comprimento.\n",
    "\n",
    "Next Sentence Prediction: Por último, a função verifica se a segunda sentença segue imediatamente a primeira. Se sim, ela adiciona um rótulo True à instância e incrementa o contador de positivos. Se não, ela adiciona um rótulo False e incrementa o contador de negativos.\n",
    "\n",
    "![DSA](imagens/bert3.png)\n",
    "\n",
    "Esta função continua gerando instâncias até que o lote esteja cheio e contenha uma quantidade igual de instâncias positivas e negativas. Então, o lote é retornado.\n",
    "\n",
    "Note que esta função é apenas um exemplo de como os dados podem ser preparados para o treinamento do BERT. Dependendo do conjunto de dados e da tarefa específica, pode ser necessário ajustar esta função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A principal inovação técnica do BERT é aplicar o treinamento bidirecional do Transformer, um modelo de atenção popular, à modelagem de linguagem. Isso contrasta com os esforços anteriores que analisavam uma sequência de texto da esquerda para a direita ou um treinamento combinado da esquerda para a direita e da direita para a esquerda. Os resultados do artigo mostram que um modelo de linguagem que é treinado bidirecionalmente pode ter um senso mais profundo de contexto e fluxo de linguagem do que modelos de linguagem de direção única. No artigo, os pesquisadores detalham uma nova técnica chamada Masked LM (MLM), que permite o treinamento bidirecional em modelos nos quais antes era impossível. Link do artigo do BERT: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fYLFtroMpydu"
   },
   "outputs": [],
   "source": [
    "# Função para criar os batches de dados\n",
    "def make_batch():\n",
    "    \n",
    "    batch = []\n",
    "    \n",
    "    positive = negative = 0\n",
    "    \n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        \n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        \n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        # MASK LM (MLM) de 15 % dos tokens em uma sentença\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) \n",
    "\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        \n",
    "        shuffle(cand_maked_pos)\n",
    "        \n",
    "        masked_tokens, masked_pos = [], []\n",
    "        \n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            \n",
    "            masked_pos.append(pos)\n",
    "            \n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            \n",
    "            if random() < 0.8:  \n",
    "                input_ids[pos] = word_dict['[MASK]'] \n",
    "            elif random() < 0.5:  \n",
    "                index = randint(0, vocab_size - 1) \n",
    "                input_ids[pos] = word_dict[number_dict[index]] \n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            \n",
    "            # IsNext\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) \n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            \n",
    "            # NotNext\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) \n",
    "            negative += 1\n",
    "            \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função get_attn_pad_masked() abaixo cria uma máscara de atenção para tokens de padding em uma sequência.\n",
    "\n",
    "Entradas: A função aceita duas sequências, seq_q e seq_k. Estas são tipicamente a sequência de consulta (query) e a sequência chave (key) em uma operação de atenção.\n",
    "\n",
    "Extração de tamanho: A função extrai o tamanho do lote (batch_size) e os comprimentos das sequências (len_q e len_k) a partir das dimensões das sequências de entrada.\n",
    "\n",
    "Criação da máscara: A máscara de atenção é criada verificando quais elementos em seq_k são iguais a zero (o que indica um token de padding). Isso produz uma matriz booleana do mesmo tamanho que seq_k, onde True indica um token de padding e False indica um token real.\n",
    "\n",
    "Adição de uma dimensão: A dimensão é adicionada à máscara usando o método unsqueeze(1), que adiciona uma dimensão extra no índice 1. Isso é necessário porque a máscara de atenção deve ter a mesma dimensão que as matrizes de atenção no Transformer.\n",
    "\n",
    "Expansão da máscara: Finalmente, a máscara é expandida para ter o mesmo tamanho que a matriz de atenção, que tem dimensões (batch_size, len_q, len_k). A máscara expandida é retornada pela função.\n",
    "\n",
    "Em resumo, a função get_attn_pad_masked cria uma máscara que pode ser usada para impedir que o modelo preste atenção aos tokens de padding quando calcula a atenção. Tokens de padding são usados para preencher sequências para que todas tenham o mesmo comprimento, mas eles não carregam nenhuma informação útil, por isso é importante garantir que o modelo os ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tun2vU1dpydv"
   },
   "outputs": [],
   "source": [
    "# Função para o padding\n",
    "def get_attn_pad_masked(seq_q, seq_k):\n",
    "    \n",
    "    batch_size, len_q = seq_q.size()\n",
    "    \n",
    "    batch_size, len_k = seq_k.size()\n",
    "    \n",
    "    pad_attn_masked = seq_k.data.eq(0).unsqueeze(1)\n",
    "    \n",
    "    return pad_attn_masked.expand(batch_size, len_q, len_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Fq2BFQ0kpydw"
   },
   "outputs": [],
   "source": [
    "# Cria um batch\n",
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "l1sOb8tbpydx"
   },
   "outputs": [],
   "source": [
    "# Extrai os elementos do batch\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGWch4cvpydx",
    "outputId": "ef366521-5f38-41eb-c170-62de1cfa8c75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
       " tensor([ 1, 37,  8, 49, 39, 16,  3, 29, 34, 57, 58, 10,  2,  3, 63, 43, 64, 20,\n",
       "         47, 38,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplica a função de padding\n",
    "get_attn_pad_masked(input_ids, input_ids)[0][0], input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do Modelo\n",
    "\n",
    "Leia o manual em pdf que descreve em detalhes as imagens abaixo.\n",
    "\n",
    "A imagem abaixo é uma descrição de alto nível do codificador Transformer. A entrada é uma sequência de tokens, que são primeiro incorporados em vetores e depois processados na rede neural. A saída é uma sequência de vetores de tamanho H, em que cada vetor corresponde a um token de entrada com o mesmo índice.\n",
    "\n",
    "Em termos técnicos, a previsão das palavras de saída requer:\n",
    "\n",
    "- 1- Adicionar uma camada de classificação na parte superior da saída do codificador.\n",
    "- 2- Multiplicar os vetores de saída pela matriz embedding, transformando-os na dimensão do vocabulário.\n",
    "- 3- Calcular a probabilidade de cada palavra no vocabulário com softmax.\n",
    "\n",
    "A função de perda no modelo BERT leva em consideração apenas a previsão dos valores mascarados e ignora a previsão das palavras não mascaradas. Como consequência, o modelo converge mais lentamente do que os modelos direcionais, uma característica que é compensada por sua maior percepção do contexto.\n",
    "\n",
    "![DSA](imagens/bert4.png)\n",
    "\n",
    "No processo de treinamento BERT, o modelo recebe pares de sentenças como entrada e aprende a prever se a segunda sentença do par é a sentença subsequente no documento original. Durante o treinamento, 50% das entradas são um par em que a segunda sentença é a sentença subsequente no documento original, enquanto nos outros 50% uma sentença aleatória do corpus é escolhida como a segunda sentença. \n",
    "\n",
    "Para ajudar o modelo a distinguir entre as duas sentenças em treinamento, a entrada é processada da seguinte maneira antes de entrar no modelo:\n",
    "\n",
    "- 1- Um token [CLS] é inserido no início da primeira frase e um token [SEP] é inserido no final de cada frase.\n",
    "- 2- Uma embedding de frase indicando a Sentença A ou a Sentença B é adicionada a cada token. As embeddings de sentença são semelhantes em conceito às embeddings de token com um vocabulário de 2.\n",
    "- 3- Uma embedding posicional é adicionada a cada token para indicar sua posição na sequência. O conceito e a implementação da embedding posicional são apresentados no artigo Transformer.\n",
    "\n",
    "De fato a embedding usada para treinar o modelo é uma combinação de várias embeddings.\n",
    "\n",
    "![DSA](imagens/bert1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de ativação GeLu\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo Embedding\n",
    "\n",
    "A classe de Embedding abaixo faz parte da arquitetura BERT. Componentes individuais da Classe:\n",
    "\n",
    "Inicialização (def init(self)): O construtor da classe inicializa os componentes necessários para as embeddings.\n",
    "\n",
    "self.tok_embed: Esta é a camada de embedding de token que mapeia cada token para um vetor de dimensão d_model.\n",
    "\n",
    "self.pos_embed: Esta é a camada de embedding de posição que mapeia a posição de um token dentro de uma sequência para um vetor de dimensão d_model.\n",
    "\n",
    "self.seg_embed: Esta é a camada de embedding de segmento que mapeia o tipo de token (0 para a primeira sentença e 1 para a segunda sentença) para um vetor de dimensão d_model.\n",
    "\n",
    "self.norm: Este é o componente de normalização da camada que é usado para normalizar os vetores de embedding.\n",
    "\n",
    "Método Forward (def forward(self, x, seg)): O método forward é onde a embedding real acontece.\n",
    "\n",
    "- Primeiro, ele calcula a posição de cada token na sequência.\n",
    "- Em seguida, ele cria uma matriz de posições da mesma forma que a entrada x usando pos.unsqueeze(0).expand_as(x).\n",
    "- Depois, ele calcula a embedding total como a soma das embeddings de token, posição e segmento.\n",
    "- Finalmente, ele normaliza a embedding usando a camada de normalização e retorna o resultado.\n",
    "\n",
    "A combinação dessas três embeddings permite ao BERT levar em consideração tanto o significado individual dos tokens quanto a ordem em que aparecem na sequência, bem como se o token pertence à primeira ou à segunda sentença. Isso torna a embedding do BERT muito poderosa e flexível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Embedding\n",
    "class Embedding(nn.Module):\n",
    "    \n",
    "    # Método construtor\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        # Token embedding\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  \n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  \n",
    "        \n",
    "        # Segment (tipo de token) embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  \n",
    "        \n",
    "        # Normalização de camada\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    # Método Forward\n",
    "    def forward(self, x, seg):\n",
    "        \n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        pos = torch.arange(seq_len, dtype = torch.long)\n",
    "        \n",
    "        # (seq_len,) -> (batch_size, seq_len)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  \n",
    "        \n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        \n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo Scaled Dot Product Attention\n",
    "\n",
    "Abaixo está a implementação do mecanismo de Atenção por Produto Escalar Normalizado (Scaled Dot-Product Attention), que é uma parte chave do modelo Transformer, utilizado no BERT e em outros modelos de processamento de linguagem natural.\n",
    "\n",
    "Aqui está uma explicação linha a linha do método forward:\n",
    "\n",
    "Pontuações (Scores): O produto escalar de Q (matriz de consulta) e K (matriz chave) é calculado para determinar a pontuação para cada par de chave-consulta. Essas pontuações determinam o quanto cada elemento da sequência de entrada deve ser atendido na produção da representação de saída para um determinado elemento. A pontuação é então escalada pela raiz quadrada da dimensão das chaves (d_k) para evitar que os valores de produto escalar se tornem muito grandes em ambientes de alta dimensão.\n",
    "\n",
    "Máscara de Atenção: A máscara de atenção é aplicada às pontuações ao preencher os locais onde a máscara tem valor 1 com um número muito grande negativo (-1e9). Isso garante que esses locais recebam um peso próximo de zero quando a softmax for aplicada.\n",
    "\n",
    "Softmax: A função softmax é aplicada ao último eixo das pontuações para obter os pesos de atenção. Isso garante que todos os pesos são positivos e somam 1, então eles podem ser interpretados como probabilidades.\n",
    "\n",
    "Contexto: Os pesos de atenção são então multiplicados pela matriz de valores V (value) para obter a saída do mecanismo de atenção. Cada valor é ponderado pela quantidade que deveríamos \"atender\" a esse valor, conforme determinado pelos pesos de atenção.\n",
    "\n",
    "O método retorna o contexto (a saída ponderada) e a matriz de atenção.\n",
    "\n",
    "No modelo Transformer, a Atenção por Produto Escalar Normalizado é usada várias vezes em cada camada, permitindo que o modelo dê atenção a diferentes partes da entrada ao produzir cada elemento da saída. Isso permite que o Transformer lide efetivamente com as dependências de longo alcance entre as palavras nas sequências de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe ScaledDotProductAttention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) \n",
    "        \n",
    "        scores.masked_fill_(attn_mask, -1e9) \n",
    "        \n",
    "        attn = nn.Softmax(dim = -1)(scores)\n",
    "        \n",
    "        context = torch.matmul(attn, V)\n",
    "        \n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo Multi-Head Attention\n",
    "\n",
    "Abaixo está a implementação de Atenção Multi-Cabeças (Multi-Head Attention), que é um componente chave da arquitetura Transformer, usada em modelos como BERT. A ideia da atenção multi-cabeças é aplicar a atenção do produto escalar normalizado várias vezes em paralelo, cada uma com diferentes pesos aprendidos. Isso permite ao modelo focar em diferentes posições e capturar vários tipos de informação.\n",
    "\n",
    "Vamos analisar linha a linha o método forward:\n",
    "\n",
    "Inicialização: residual e batch_size são inicializados com Q e o tamanho do primeiro eixo de Q, respectivamente. O residual será utilizado mais tarde para o caminho de conexão residual.\n",
    "\n",
    "Transformações Lineares: Aplicamos transformações lineares aos dados de entrada (Q, K e V) usando pesos diferentes. Essas transformações geram múltiplas \"cabeças\" de atenção.\n",
    "\n",
    "Remodelagem: As saídas dessas transformações lineares são então reformuladas e transpostas para terem a forma apropriada para a atenção de produto escalar normalizado.\n",
    "\n",
    "Máscara de Atenção: A máscara de atenção é ajustada para corresponder ao formato das cabeças de atenção.\n",
    "\n",
    "Atenção de Produto Escalar Normalizado: A atenção de produto escalar normalizado é então aplicada a cada uma das cabeças de atenção.\n",
    "\n",
    "Remodelagem do Contexto: A saída (contexto) de cada cabeça de atenção é então reformulada e concatenada.\n",
    "\n",
    "Transformação Linear e Normalização: Uma transformação linear é aplicada ao contexto concatenado, seguida de uma normalização de camada.\n",
    "\n",
    "Conexão Residual: O resultado final é obtido somando a saída da normalização de camada ao caminho de conexão residual (entrada original Q).\n",
    "\n",
    "Finalmente, a função retorna a saída normalizada e a matriz de atenção. A atenção multi-cabeças permite que o modelo considere informações de diferentes partes da sequência de entrada, em diferentes subespaços de representação, ao mesmo tempo, o que melhora a capacidade do modelo de capturar várias características do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FEMcW2aEpydy"
   },
   "outputs": [],
   "source": [
    "# Classe MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        \n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        \n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        \n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        \n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)\n",
    "        \n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
    "        \n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        \n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        \n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        \n",
    "        return nn.LayerNorm(d_model)(output + residual), attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto Embedding\n",
    "emb = Embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera as Embeddings\n",
    "embeds = emb(input_ids, segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera a máscara de atenção\n",
    "attenM = get_attn_pad_masked(input_ids, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera o MultiHeadAttention\n",
    "MHA = MultiHeadAttention()(embeds, embeds, embeds, attenM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYpEq8Kgpydy",
    "outputId": "fc0b2840-f834-4c29-dbcf-0a9b1d017438"
   },
   "outputs": [],
   "source": [
    "# Saída\n",
    "output, A = MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0359, 0.0653, 0.0654,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0555, 0.0341, 0.0608,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0319, 0.0762, 0.0410,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0231, 0.0543, 0.0417,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0250, 0.0428, 0.0387,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0291, 0.0395, 0.0358,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo Feedforward Posicional\n",
    "\n",
    "Esta é a implementação da Rede Feedforward Posicional (PoswiseFeedForward), que é um componente da arquitetura Transformer, utilizada em modelos como o BERT.\n",
    "\n",
    "A Rede Feedforward Posicional é composta por duas camadas lineares com uma ativação GELU (Gaussian Error Linear Unit) entre elas.\n",
    "\n",
    "Aqui está a explicação detalhada do método forward:\n",
    "\n",
    "Primeira Camada Linear (self.fc1): A entrada x é passada por uma camada linear (também chamada de camada totalmente conectada). Essa camada tem uma transformação linear com d_model entradas e d_ff saídas, onde d_model é a dimensão do espaço de incorporação e d_ff é a dimensão da camada oculta da rede feed-forward. Isso permite ao modelo aprender representações não-lineares.\n",
    "\n",
    "Ativação GELU: Em seguida, a ativação GELU é aplicada. A função GELU permite ao modelo aprender transformações mais complexas e não lineares. Ela ajuda a lidar com o problema do desaparecimento do gradiente, permitindo que mais informações passem através da rede.\n",
    "\n",
    "Segunda Camada Linear (self.fc2): Por fim, a saída da ativação GELU é passada por uma segunda camada linear, que transforma a saída de volta para a dimensão original d_model. Isso é feito para que a saída desta rede feedforward possa ser somada à entrada original (conexão residual) no Transformer.\n",
    "\n",
    "O retorno da função é, portanto, a saída dessa segunda camada linear, que passou pela transformação da primeira camada linear, ativação GELU, e segunda camada linear.\n",
    "\n",
    "As redes feed-forward posicionais são uma parte importante dos modelos Transformer, permitindo-lhes aprender representações mais complexas e fazer transformações não-lineares dos dados de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe PoswiseFeedForward\n",
    "class PoswiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super(PoswiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo Encoder Layer\n",
    "\n",
    "Esta classe define uma Camada de Codificador (EncoderLayer), que é um componente da arquitetura Transformer e também é usado em modelos como BERT. Cada camada de codificador no Transformer contém duas subcamadas: uma camada de Atenção Multi-Cabeças e uma Rede Feed-Forward Posicional.\n",
    "\n",
    "Aqui está a explicação detalhada do método forward:\n",
    "\n",
    "Atenção Multi-Cabeças (self.enc_self_attn): A entrada enc_inputs passa por uma camada de Atenção Multi-Cabeças, que é usada para que cada palavra na entrada tenha atenção direcionada a todas as outras palavras. Essa camada também recebe uma máscara (enc_self_attn_mask), que é usada para evitar que o modelo preste atenção a certas palavras (como as de preenchimento). A saída da Atenção Multi-Cabeças é outra sequência de representações vetoriais, com a mesma dimensão da entrada. A matriz de atenção que mostra como cada palavra se atentou a todas as outras também é retornada.\n",
    "\n",
    "Rede Feed-Forward Posicional (self.pos_ffn): A saída da camada de Atenção Multi-Cabeças passa então por uma Rede Feed-Forward Posicional. Esta é uma rede neural simples que opera independentemente em cada posição da sequência (ou seja, a mesma rede é aplicada a cada posição). Isso permite ao modelo aprender representações mais complexas e realizar transformações não-lineares dos dados.\n",
    "\n",
    "A função retorna a saída desta camada de codificador, que é a saída da Rede Feed-Forward Posicional, junto com a matriz de atenção. Portanto, a entrada e a saída desta camada do codificador têm a mesma dimensão, o que permite que várias dessas camadas de codificador sejam empilhadas para formar o codificador completo do Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "f6Eij7fGpydz"
   },
   "outputs": [],
   "source": [
    "# Classe EncoderLayer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        \n",
    "        self.pos_ffn = PoswiseFeedForward()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        \n",
    "        enc_inputs, atnn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "        \n",
    "        enc_inputs = self.pos_ffn(enc_inputs)\n",
    "        \n",
    "        return enc_inputs, atnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo BERT\n",
    "\n",
    "Esta classe define o modelo BERT (Bidirectional Encoder Representations from Transformers), um modelo de linguagem de última geração que usa transformers e atenção bidirecional para entender a semântica das palavras dentro de um contexto.\n",
    "\n",
    "Vamos analisar em detalhes o método forward:\n",
    "\n",
    "Embedding (self.embedding): Transforma as entradas (input_ids e segment_ids) em vetores densos (embeddings).\n",
    "\n",
    "Máscara de Atenção (get_attn_pad_masked): Gera uma máscara de atenção para ignorar os tokens de preenchimento (pad) nas entradas.\n",
    "\n",
    "Camadas de Codificação (self.layers): Passa a saída do embedding e a máscara de atenção através de várias camadas do codificador. Cada camada de codificador é composta por uma camada de atenção multi-cabeças e uma rede feed-forward posicional.\n",
    "\n",
    "Pooling (self.activ1(self.fc(output[:, 0]))): Aplica uma camada totalmente conectada e uma ativação tangente hiperbólica à primeira posição (o token de classificação) de cada sequência na saída do codificador. Isso resulta em um vetor de representação de sequência.\n",
    "\n",
    "Classificador (self.classifier): Uma camada totalmente conectada que gera os logits para a tarefa de classificação de próxima sentença.\n",
    "\n",
    "Extração de Tokens Mascarados (torch.gather(output, 1, masked_pos)): Selecione os vetores de saída correspondentes aos tokens mascarados.\n",
    "\n",
    "Transformação dos Tokens Mascarados (self.norm(self.activ2(self.linear(h_masked)))): Aplica uma transformação linear, uma ativação GELU e normalização à saída dos tokens mascarados.\n",
    "\n",
    "Decoder (self.decoder): Uma camada linear que gera os logits para a tarefa de modelagem de linguagem mascarada. Usa os mesmos pesos que a camada de embedding de tokens para a consistência no espaço de representação. Esta função decoder é usada somente para gerar os logits finais e não é usada no processo de aprendizado do modelo.\n",
    "\n",
    "O método retorna os logits para a tarefa de modelagem de linguagem mascarada e a tarefa de classificação de próxima sentença. Esses logits podem então ser usados para calcular as perdas para ambas as tarefas durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-hstWm_rpydz"
   },
   "outputs": [],
   "source": [
    "# Modelo BERT\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \n",
    "        super(BERT, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding()\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.activ1 = nn.Tanh()\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.activ2 = gelu\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        \n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        \n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        \n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        \n",
    "        self.decoder.weight = embed_weight\n",
    "        \n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        \n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        \n",
    "        enc_self_attn_mask = get_attn_pad_masked(input_ids, input_ids)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        \n",
    "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
    "        \n",
    "        logits_clsf = self.classifier(h_pooled)\n",
    "        \n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
    "        \n",
    "        h_masked = torch.gather(output, 1, masked_pos)\n",
    "        \n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        \n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        \n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento e Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o modelo\n",
    "model = BERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de erro\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizador\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo está o ciclo típico de treinamento de uma época em um modelo de aprendizado de máquina. Vamos quebrá-lo em etapas:\n",
    "\n",
    "optimizer.zero_grad(): Zera os gradientes de todas as variáveis otimizadas. Isso é feito porque os gradientes no PyTorch são acumulados, ou seja, cada vez que chamamos .backward(), os gradientes são somados em vez de substituídos. Então, precisamos limpar esses gradientes acumulados antes de cada passo de otimização.\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos): Alimenta os dados de entrada no modelo e obtém a saída do modelo. A saída é composta de logits_lm e logits_clsf, que são os resultados brutos não normalizados para a tarefa de modelagem de linguagem e a tarefa de classificação, respectivamente.\n",
    "\n",
    "loss_lm = criterion(logits_lm.transpose(1,2), masked_tokens): Calcula a perda da tarefa de modelagem de linguagem mascarada. criterion é a função de perda, logits_lm.transpose(1,2) são as previsões do modelo e masked_tokens são os alvos verdadeiros.\n",
    "\n",
    "loss_lm = (loss_lm.float()).mean(): Converte a perda em um tipo de dados de ponto flutuante (se ainda não for) e, em seguida, calcula a média da perda.\n",
    "\n",
    "loss_clsf = criterion(logits_clsf, isNext): Calcula a perda da tarefa de classificação da próxima frase.\n",
    "\n",
    "loss = loss_lm + loss_clsf: Combina as duas perdas em uma única perda escalar.\n",
    "\n",
    "loss.backward(): Calcula os gradientes de todas as variáveis otimizadas. Os gradientes são computados com respeito à perda.\n",
    "\n",
    "optimizer.step(): Atualiza os parâmetros do modelo usando os gradientes calculados.\n",
    "\n",
    "Essas etapas são repetidas para cada época de treinamento. Cada época é um ciclo completo através do conjunto de treinamento. Portanto, se NUM_EPOCHS é 10, então o processo de treinamento completo é executado 10 vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "OtgfwJ17pyd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss 67.1648\n",
      "Epoch: 2 | Loss 83.9593\n",
      "Epoch: 3 | Loss 296.9586\n",
      "Epoch: 4 | Loss 133.0423\n",
      "Epoch: 5 | Loss 53.5106\n",
      "Epoch: 6 | Loss 34.0669\n",
      "Epoch: 7 | Loss 31.3566\n",
      "Epoch: 8 | Loss 19.9872\n",
      "Epoch: 9 | Loss 35.3451\n",
      "Epoch: 10 | Loss 38.6164\n",
      "Epoch: 11 | Loss 18.6592\n",
      "Epoch: 12 | Loss 21.3668\n",
      "Epoch: 13 | Loss 26.1293\n",
      "Epoch: 14 | Loss 28.1905\n",
      "Epoch: 15 | Loss 28.2236\n",
      "Epoch: 16 | Loss 25.7394\n",
      "Epoch: 17 | Loss 25.0417\n",
      "Epoch: 18 | Loss 22.9130\n",
      "Epoch: 19 | Loss 20.8714\n",
      "Epoch: 20 | Loss 24.3999\n",
      "Epoch: 21 | Loss 19.4341\n",
      "Epoch: 22 | Loss 18.4059\n",
      "Epoch: 23 | Loss 18.5944\n",
      "Epoch: 24 | Loss 19.4257\n",
      "Epoch: 25 | Loss 18.0532\n",
      "Epoch: 26 | Loss 18.6622\n",
      "Epoch: 27 | Loss 19.3121\n",
      "Epoch: 28 | Loss 17.8534\n",
      "Epoch: 29 | Loss 17.9634\n",
      "Epoch: 30 | Loss 21.3748\n",
      "Epoch: 31 | Loss 18.0867\n",
      "Epoch: 32 | Loss 16.7785\n",
      "Epoch: 33 | Loss 18.7112\n",
      "Epoch: 34 | Loss 18.5004\n",
      "Epoch: 35 | Loss 17.7398\n",
      "Epoch: 36 | Loss 18.4515\n",
      "Epoch: 37 | Loss 17.5236\n",
      "Epoch: 38 | Loss 16.7070\n",
      "Epoch: 39 | Loss 16.5959\n",
      "Epoch: 40 | Loss 14.8597\n",
      "Epoch: 41 | Loss 14.1076\n",
      "Epoch: 42 | Loss 13.1173\n",
      "Epoch: 43 | Loss 13.0697\n",
      "Epoch: 44 | Loss 13.8231\n",
      "Epoch: 45 | Loss 11.5332\n",
      "Epoch: 46 | Loss 12.3352\n",
      "Epoch: 47 | Loss 11.3782\n",
      "Epoch: 48 | Loss 10.4015\n",
      "Epoch: 49 | Loss 9.2919\n",
      "Epoch: 50 | Loss 9.7993\n"
     ]
    }
   ],
   "source": [
    "# Loop de treino\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    \n",
    "    loss_lm = criterion(logits_lm.transpose(1,2), masked_tokens)\n",
    "    \n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    \n",
    "    loss_clsf = criterion(logits_clsf, isNext)\n",
    "    \n",
    "    loss = loss_lm + loss_clsf\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1} | Loss {loss:.4f}')\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo as Previsões do Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Olá, como vai? Eu sou a Ana.\\n'\n",
      "'Olá, Ana, meu nome é Carlos. Muito prazer.\\n'\n",
      "'Prazer em conhecer você também. Como você está hoje?\\n'\n",
      "'Ótimo. Meu time de futebol venceu a competição.\\n'\n",
      "'Uau Parabéns, Carlos!\\n'\n",
      "'Obrigado Ana.\\n'\n",
      "'Vamos comer uma pizza mais tarde para celebrar?\\n'\n",
      "'Claro. Você recomenda algum restaurante Ana?\\n'\n",
      "'Sim, abriu um restaurante novo e dizem que a pizza de banana é fenomenal.\\n'\n",
      "'Ok. Nos encontramos no restaurante às sete da noite, pode ser?\\n'\n",
      "'Pode sim. Nos vemos mais tarde então.'\n",
      "['[CLS]', \"'olá\", '[MASK]', 'vai', 'eu', 'sou', 'a', \"ana\\\\n'\", '[SEP]', \"'ok\", 'nos', 'encontramos', 'no', 'restaurante', 'às', 'sete', '[MASK]', 'noite', 'pode', 'conhecer', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Extrai o batch\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de Masked Tokens Reais:  [10, 63, 34]\n",
      "Lista de Masked Tokens Previstos:  [59, 59]\n"
     ]
    }
   ],
   "source": [
    "# Extrai as previsões dos tokens\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('Lista de Masked Tokens Reais: ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('Lista de Masked Tokens Previstos: ', [pos for pos in logits_lm if pos != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsAhgTZCpyd0",
    "outputId": "ccd4cae3-835d-4678-da37-41018d77354f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isNext (Valor Real):  False\n",
      "isNext (Valor Previsto):  False\n"
     ]
    }
   ],
   "source": [
    "# Extrai as previsões do próximo token\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext (Valor Real): ', True if isNext else False)\n",
    "print('isNext (Valor Previsto): ', True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
