{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning Para Aplicações de IA com PyTorch e Lightning</font>\n",
    "\n",
    "## <font color='blue'>Mini-Projeto 3 - Deploy</font>\n",
    "## <font color='blue'>Fine-Tuning de Modelo LLM Para Tarefa Específica e Deploy de Web App com Gradio</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DSA](imagens/MP3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.10.9\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/gradio/\n",
    "!pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "torch : 2.0.1\n",
      "gradio: 3.39.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo\n",
    "modelo_llm = AutoModelForCausalLM.from_pretrained(\"modelos/modelo_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada NumberTokenizer, que é usada para tokenizar os números\n",
    "class DSATokenizer:\n",
    "    \n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, numbers_qty = 10):\n",
    "        \n",
    "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
    "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "        \n",
    "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
    "        self.numbers_qty = numbers_qty\n",
    "        \n",
    "        # Definindo o token de preenchimento (padding)\n",
    "        self.pad_token = '-1'\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada token para um índice único\n",
    "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
    "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Obtendo o índice do token de preenchimento no encoder\n",
    "        self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
    "    def __call__(self, text):\n",
    "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
    "        return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto\n",
    "tokenizer = DSATokenizer(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
    "def faz_previsao(entrada, solution_length = 6, model = modelo_llm):\n",
    "\n",
    "    # Colocando o modelo em modo de avaliação. \n",
    "    model.eval()\n",
    "\n",
    "    # Convertendo a entrada (string) em tensor utilizando o tokenizer. \n",
    "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
    "    entrada = torch.tensor(tokenizer(entrada))\n",
    "\n",
    "    # Iniciando uma lista vazia para armazenar a solução\n",
    "    solution = []\n",
    "\n",
    "    # Loop que gera a solução de comprimento solution_length\n",
    "    for i in range(solution_length):\n",
    "\n",
    "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
    "        saida = model(entrada)\n",
    "\n",
    "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída, \n",
    "        # que é a previsão do modelo para o próximo token\n",
    "        predicted = saida.logits[-1].argmax()\n",
    "\n",
    "        # Concatenando a previsão atual com a entrada atual. \n",
    "        # Isso servirá como a nova entrada para a próxima iteração.\n",
    "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
    "\n",
    "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
    "        solution.append(predicted.cpu().item())\n",
    "\n",
    "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
    "    return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testa a função\n",
    "faz_previsao('3 + 5 =', solution_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para retornar a função que faz a previsão\n",
    "def funcsolve(entrada):\n",
    "    return faz_previsao(entrada, solution_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a web app\n",
    "webapp = gr.Interface(fn = funcsolve, \n",
    "                      inputs = [gr.Textbox(label = \"Dados de Entrada\", \n",
    "                                           lines = 1, \n",
    "                                           info = \"Os dados devem estar na forma: '1 + 2 =' com um único espaço entre cada caractere e apenas números de um dígito são permitidos.\")],\n",
    "                      outputs = [gr.Textbox(label = \"Resultado (Previsão do Modelo)\", lines = 1)],\n",
    "                      title = \"Deploy de LLM Após o Fine-Tuning\",\n",
    "                      description = \"Digite os dados de entrada e clique no botão Submit para o modelo fazer a previsão.\",\n",
    "                      examples = [\"5 + 3 =\", \"2 + 9 =\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/gradio/routes.py\", line 442, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1392, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/gradio/blocks.py\", line 1097, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/anyio/to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/dmpm/anaconda3/lib/python3.10/site-packages/gradio/utils.py\", line 703, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/dc/lqrc3k5j4438r150cbrdr_000000gn/T/ipykernel_5062/1694334565.py\", line 3, in funcsolve\n",
      "    return faz_previsao(entrada, solution_length = 2)\n",
      "  File \"/var/folders/dc/lqrc3k5j4438r150cbrdr_000000gn/T/ipykernel_5062/2965097493.py\", line 9, in faz_previsao\n",
      "    entrada = torch.tensor(tokenizer(entrada))\n",
      "  File \"/var/folders/dc/lqrc3k5j4438r150cbrdr_000000gn/T/ipykernel_5062/3426772234.py\", line 32, in __call__\n",
      "    return [self.encoder[t] for t in text.split()]\n",
      "  File \"/var/folders/dc/lqrc3k5j4438r150cbrdr_000000gn/T/ipykernel_5062/3426772234.py\", line 32, in <listcomp>\n",
      "    return [self.encoder[t] for t in text.split()]\n",
      "KeyError: '122220'\n"
     ]
    }
   ],
   "source": [
    "webapp.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
