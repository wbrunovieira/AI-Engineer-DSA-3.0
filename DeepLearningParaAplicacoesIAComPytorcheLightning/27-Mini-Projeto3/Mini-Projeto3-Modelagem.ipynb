{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Deep Learning Para Aplicações de IA com PyTorch e Lightning</font>\n",
    "\n",
    "## <font color='blue'>Mini-Projeto 3 - Modelagem</font>\n",
    "## <font color='blue'>Fine-Tuning de Modelo LLM Para Tarefa Específica e Deploy de Web App com Gradio</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DSA](imagens/MP3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando e Carregando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.10.9\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# !pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_CPP_MIN_LOG_LEVEL=3\n"
     ]
    }
   ],
   "source": [
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, GPT2Tokenizer\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "numpy       : 1.23.5\n",
      "torch       : 2.0.1\n",
      "transformers: 4.31.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o LLM\n",
    "\n",
    "https://huggingface.co/gpt2\n",
    "\n",
    "O modelo terá a mesma arquitetura do GPT-2, mas com algumas modificações para torná-lo menor. As principais mudanças são o tamanho do vocabulário que é 13 porque só vai lidar com números mais o padding token, o \"+\" e o \"=\". A janela de contexto suportará apenas 6 tokens, pois estamos interessados apenas em realizar a adição de dois dígitos únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário\n",
    "vocab_size = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento da sequência\n",
    "sequence_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento do resultado\n",
    "result_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprimento do contexto\n",
    "context_length = sequence_length + result_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de configuração do modelo GPT-2\n",
    "config = AutoConfig.from_pretrained(\"gpt2\", \n",
    "                                    vocab_size = vocab_size, \n",
    "                                    n_ctx = context_length, \n",
    "                                    n_head = 4, \n",
    "                                    n_layer = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o modelo\n",
    "modelo = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular o tamanho do modelo\n",
    "def model_size(model):\n",
    "    return sum(t.numel() for t in modelo.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do Modelo: 15.0M parâmetros\n"
     ]
    }
   ],
   "source": [
    "print(f'Tamanho do Modelo: {model_size(modelo)/1000**2:.1f}M parâmetros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo tem 15 milhões de parâmetros em vez dos 111 milhões de parâmetros da configuração padrão \"gpt2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo em disco\n",
    "modelo.save_pretrained(\"modelos/modelo_inicial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizador Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada NumberTokenizer, que é usada para tokenizar os números\n",
    "class DSATokenizer:\n",
    "    \n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, numbers_qty = 10):\n",
    "        \n",
    "        # Lista de tokens possíveis que o tokenizador pode encontrar\n",
    "        vocab = ['+', '=', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "        \n",
    "        # Definindo a quantidade de números que o tokenizador pode lidar\n",
    "        self.numbers_qty = numbers_qty\n",
    "        \n",
    "        # Definindo o token de preenchimento (padding)\n",
    "        self.pad_token = '-1'\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada token para um índice único\n",
    "        self.encoder = {str(v):i for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Criando um dicionário que mapeia cada índice único de volta ao token correspondente\n",
    "        self.decoder = {i:str(v) for i,v in enumerate(vocab)}\n",
    "        \n",
    "        # Obtendo o índice do token de preenchimento no encoder\n",
    "        self.pad_token_id = self.encoder[self.pad_token]\n",
    "\n",
    "    # Método para decodificar uma lista de IDs de token de volta para uma string\n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join(self.decoder[t] for t in token_ids)\n",
    "\n",
    "    # Método que é chamado quando o objeto da classe é invocado como uma função\n",
    "    def __call__(self, text):\n",
    "        # Dividindo o texto em tokens individuais e retornando uma lista dos IDs correspondentes\n",
    "        return [self.encoder[t] for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vamos testar o tokenizador!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto do tokenizador\n",
    "tokenizer = DSATokenizer(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '+',\n",
       " 1: '=',\n",
       " 2: '-1',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decoder do tokenizador\n",
    "tokenizer.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 4, 1, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando o tokenizador\n",
    "tokenizer(\"1 + 1 = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Dataset\n",
    "\n",
    "O conjunto de dados deve ser criado neste formato:\n",
    "\n",
    "- Entrada: \"2 + 3 = 0\" onde os 4 primeiros caracteres representam a sequência de entrada e o quinto caractere representa o primeiro caracter da saída.\n",
    "\n",
    "- Saída: \"+ 3 = 0 5\" onde os 2 últimos dígitos representam o resultado da adição e os 3 primeiros dígitos são ignorados durante o treinamento e preenchidos com o pad.\n",
    "\n",
    "O resultado é um conjunto de dados de sequências tokenizadas de números."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma classe chamada CriaDataset, que herda da classe Dataset do PyTorch\n",
    "class CriaDataset(Dataset):\n",
    "\n",
    "    # Método construtor da classe, que é executado quando um objeto dessa classe é criado\n",
    "    def __init__(self, split, length = 6):\n",
    "        \n",
    "        # Verificando se a divisão do dataset (split) é 'treino' ou 'teste'\n",
    "        assert split in {'treino', 'teste'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    # Definindo o método len que retorna o tamanho do dataset. \n",
    "    # Nesse caso, o tamanho é fixo e igual a 1 milhão.\n",
    "    def __len__(self):\n",
    "        return 1000000 \n",
    "\n",
    "    # Definindo o método getitem que é usado para obter um item específico do dataset\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Criando uma lista com todos os números disponíveis que não são tokens de padding e são numéricos\n",
    "        available_numbers = [int(n) for n in tokenizer.decoder.values() if n != tokenizer.pad_token and str(n).isnumeric()]\n",
    "        \n",
    "        # Selecionando aleatoriamente números da lista de números disponíveis para criar uma entrada (input)\n",
    "        inp = torch.tensor(np.random.choice(available_numbers, size = result_length))\n",
    "        \n",
    "        # Calculando a soma dos números selecionados e criando um tensor\n",
    "        sol = torch.tensor([int(i) for i in str(inp.sum().item())])\n",
    "        \n",
    "        # Preenchendo o tensor com zeros para que tenha o tamanho desejado\n",
    "        sol = torch.nn.functional.pad(sol, (1 if sol.size()[0] == 1 else 0,0), 'constant', 0)\n",
    "\n",
    "        # Concatenando a entrada e a solução em um tensor\n",
    "        cat = torch.cat((inp, sol), dim = 0)\n",
    "\n",
    "        # Criando os tensores de entrada e alvo para o treinamento do modelo\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "\n",
    "        # Definindo o primeiro elemento do tensor alvo como o token de padding\n",
    "        y[:1] = int(tokenizer.pad_token)\n",
    "\n",
    "        # Transformando os tensores x e y em strings\n",
    "        x = str(x[0].item()) + ' + ' + str(x[1].item()) + ' = ' + str(x[2].item())\n",
    "        y = '-1 ' + str(y[0].item()) + ' -1 ' + str(y[1].item()) + ' ' + str(y[2].item())\n",
    "        \n",
    "        # Tokenizando as strings de entrada e alvo\n",
    "        tokenized_input = tokenizer(x)\n",
    "        tokenized_output = tokenizer(y)\n",
    "        \n",
    "        # Retornando os tensores de entrada e alvo como itens do dataset\n",
    "        return torch.tensor(tokenized_input), torch.tensor(tokenized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino = CriaDataset('treino', length = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_teste = CriaDataset('teste', length = sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset_treino[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  0,  4,  1,  3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  2,  2,  3, 11])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 + 1 = 0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 -1 -1 0 8\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(y.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(modelo.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = torch.utils.data.DataLoader(dataset_treino, shuffle = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/accelerate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "numpy       : 1.23.5\n",
      "accelerate  : 0.21.0\n",
      "torch       : 2.0.1\n",
      "transformers: 4.31.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo, optimizer, dados = accelerator.prepare(modelo, optimizer, dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(13, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-1): 2 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=13, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 --- Erro: 0.10058706998825073\n",
      "Epoch: 2/2 --- Erro: 0.05222655460238457\n",
      "CPU times: user 6min 51s, sys: 899 ms, total: 6min 52s\n",
      "Wall time: 6min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iniciando o loop para as épocas de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Iterando por cada batch (conjunto) de dados de entrada e alvos no dataset de treinamento\n",
    "    for source, targets in dados:\n",
    "\n",
    "        # Resetando os gradientes acumulados no otimizador\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculando a perda (loss) através da entropia cruzada entre as previsões do modelo e os alvos verdadeiros. \n",
    "        # Os tensores são \"achatados\" para que possam ser passados para a função de entropia cruzada. \n",
    "        # O índice do token de preenchimento (pad_token) é ignorado no cálculo da perda.\n",
    "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1), \n",
    "                               targets.flatten(end_dim = 1), \n",
    "                               ignore_index = tokenizer.pad_token_id)\n",
    "\n",
    "        # Calculando os gradientes da perda em relação aos parâmetros do modelo\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        # Atualizando os parâmetros do modelo utilizando os gradientes calculados\n",
    "        optimizer.step()\n",
    "\n",
    "        # Recalculando a perda após a etapa de otimização. \n",
    "        loss = F.cross_entropy(modelo(source).logits.flatten(end_dim = 1), \n",
    "                               targets.flatten(end_dim = 1), \n",
    "                               ignore_index = tokenizer.pad_token_id)\n",
    "\n",
    "    # Imprimindo a época atual e a perda após cada época de treinamento\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs} --- Erro: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função gera_solution com três parâmetros: input, solution_length e model\n",
    "def faz_previsao(entrada, solution_length = 6, model = modelo):\n",
    "\n",
    "    # Colocando o modelo em modo de avaliação. \n",
    "    model.eval()\n",
    "\n",
    "    # Convertendo a entrada (string) em tensor utilizando o tokenizer. \n",
    "    # O tensor é uma estrutura de dados que o modelo de aprendizado de máquina pode processar.\n",
    "    entrada = torch.tensor(tokenizer(entrada))\n",
    "\n",
    "    # Enviando o tensor de entrada para o dispositivo de cálculo disponível (CPU ou GPU)\n",
    "    entrada = entrada.to(accelerator.device)\n",
    "\n",
    "    # Iniciando uma lista vazia para armazenar a solução\n",
    "    solution = []\n",
    "\n",
    "    # Loop que gera a solução de comprimento solution_length\n",
    "    for i in range(solution_length):\n",
    "\n",
    "        # Alimentando a entrada atual ao modelo e obtendo a saída\n",
    "        saida = model(entrada)\n",
    "\n",
    "        # Pegando o índice do maior valor no último conjunto de logits (log-odds) da saída, \n",
    "        # que é a previsão do modelo para o próximo token\n",
    "        predicted = saida.logits[-1].argmax()\n",
    "\n",
    "        # Concatenando a previsão atual com a entrada atual. \n",
    "        # Isso servirá como a nova entrada para a próxima iteração.\n",
    "        entrada = torch.cat((entrada, predicted.unsqueeze(0)), dim = 0)\n",
    "\n",
    "        # Adicionando a previsão atual à lista de soluções e convertendo o tensor em um número Python padrão\n",
    "        solution.append(predicted.cpu().item())\n",
    "\n",
    "    # Decodificando a lista de soluções para obter a string de saída e retornando-a\n",
    "    return tokenizer.decode(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a função avalia_modelo com dois parâmetros: num_samples e log\n",
    "def avalia_modelo(num_samples = 1000, log = False):\n",
    "\n",
    "    # Iniciando um contador para as previsões corretas\n",
    "    correct = 0\n",
    "\n",
    "    # Loop que itera num_samples vezes\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        # Obtendo a entrada e o alvo (resposta correta) do i-ésimo exemplo do conjunto de teste\n",
    "        entrada, target = dataset_teste[i]\n",
    "\n",
    "        # Convertendo os tensores de entrada e alvo em arrays numpy para processamento posterior\n",
    "        entrada = entrada.cpu().numpy()\n",
    "        target = target.cpu().numpy()\n",
    "\n",
    "        # Decodificando a entrada e o alvo utilizando o tokenizer\n",
    "        entrada = tokenizer.decode(entrada[:sequence_length])\n",
    "        target = tokenizer.decode(target[sequence_length-1:])\n",
    "\n",
    "        # Gerando a previsão utilizando a função faz_previsao\n",
    "        predicted = faz_previsao(entrada, solution_length = result_length, model = modelo)\n",
    " \n",
    "        # Se a previsão for igual ao alvo, incrementa o contador de previsões corretas\n",
    "        if target == predicted:\n",
    "            correct += 1\n",
    "            # Se log for True, imprime detalhes do exemplo e a previsão correta\n",
    "            if log:\n",
    "                print(f'Acerto do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
    "        else:\n",
    "            # Se log for True, imprime detalhes do exemplo e a previsão errada\n",
    "            if log:\n",
    "                print(f'Erro do Modelo: Input: {entrada} Target: {target} Previsão: {predicted}')\n",
    "\n",
    "    # Ao final do loop, calcula a acurácia (número de previsões corretas dividido pelo número total de exemplos) \n",
    "    print(f'Acurácia: {correct/num_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acerto do Modelo: Input: 0 + 1 = Target: 0 1 Previsão: 0 1\n",
      "Acerto do Modelo: Input: 0 + 3 = Target: 0 3 Previsão: 0 3\n",
      "Acerto do Modelo: Input: 1 + 8 = Target: 0 9 Previsão: 0 9\n",
      "Acerto do Modelo: Input: 2 + 4 = Target: 0 6 Previsão: 0 6\n",
      "Acerto do Modelo: Input: 2 + 4 = Target: 0 6 Previsão: 0 6\n",
      "Acerto do Modelo: Input: 5 + 8 = Target: 1 3 Previsão: 1 3\n",
      "Acerto do Modelo: Input: 1 + 4 = Target: 0 5 Previsão: 0 5\n",
      "Acerto do Modelo: Input: 8 + 9 = Target: 1 7 Previsão: 1 7\n",
      "Acerto do Modelo: Input: 5 + 9 = Target: 1 4 Previsão: 1 4\n",
      "Acerto do Modelo: Input: 2 + 4 = Target: 0 6 Previsão: 0 6\n",
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Executa a função\n",
    "avalia_modelo(num_samples = 10, log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Executa a função\n",
    "avalia_modelo(num_samples = 1000, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.save_pretrained(\"modelos/modelo_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
